Spotify Tracks Genre (train.csv):
The file train.csv contains the training data for building machine learning models using the Spotify Tracks Genre Dataset.


Dimensions after processing/cleaning: 1600 x 16
artists
Type: String
Description: The name(s) of the artist(s) associated with the track. If there are multiple artists, they are listed as a comma-separated string.
album_name
Type: String
Description: The title of the album that the track belongs to. Helps identify the release context of the song.
track_name
Type: String
Description: The official name or title of the track.
popularity
Type: Integer (0 – 100)
Description: A score provided by Spotify that reflects how popular a track is. Higher values indicate greater popularity based on streaming volume and user engagement.
duration_ms
Type: Integer
Description: The length of the track in milliseconds. Useful for computing song duration in minutes or seconds.
explicit
Type: Boolean
Description: Indicates whether the track contains explicit content (e.g., profanity, adult themes). True means explicit; False means clean.
track_genre
Type: String
Description: The genre label associated with the track, selected from 125 unique genres in the dataset.
danceability
Type: Float (0.0 – 1.0)
Description: Measures how suitable a track is for dancing based on tempo, rhythm, and beat strength. Higher values = more danceable.
energy
Type: Float (0.0 – 1.0)
Description: Describes the intensity and activity level of a track. High energy tracks feel fast, loud, and aggressive.
key
Type: Integer (0 – 11)
Description: The estimated musical key of the track. Each number corresponds to a pitch class (e.g., 0 = C, 1 = C♯/D♭, … , 11 = B).
loudness
Type: Float (decibels)
Description: The average loudness of the track in dB, usually ranging from -60 to 0. Higher values indicate louder tracks.
mode
Type: Integer (0 or 1)
Description: Indicates the modality (tonal mode) of the track. 1 = major (often perceived as happy), 0 = minor (often perceived as sad).
speechiness
Type: Float (0.0 – 1.0)
Description: Measures the presence of spoken words. Higher values suggest more speech content (e.g., podcasts, rap).
acousticness
Type: Float (0.0 – 1.0)
Description: A confidence measure of whether a track is acoustic. Closer to 1.0 means a higher likelihood of acoustic sound.
instrumentalness
Type: Float (0.0 – 1.0)
Description: Predicts whether a track is likely instrumental (i.e., no vocals). Values closer to 1.0 suggest instrumental tracks.
liveness
Type: Float (0.0 – 1.0)
Description: Detects the presence of a live audience. Tracks with values above 0.8 are strongly likely to be live recordings.
valence
Type: Float (0.0 – 1.0)
Description: Describes the musical positivity expressed in a track. High valence = happy/cheerful; low valence = sad or angry.
tempo
Type: Float (beats per minute)
Description: The overall tempo (pace) of a track in BPM. Helps measure how fast or slow a song is.
time_signature
Type: Integer
Description: The estimated number of beats per bar (musical measure). Most tracks have a time signature of 3, 4, or 5.